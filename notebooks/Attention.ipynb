{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join('../data/mn40','metadata_modelnet40.csv'))['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Input, Concatenate, Layer, Dense, TimeDistributed, \n",
    "                                     MultiHeadAttention, GlobalMaxPooling1D, LSTM, \n",
    "                                     LayerNormalization, Dropout, Flatten, Bidirectional)\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MN40Loader(Sequence):\n",
    "    def __init__(self, data_root, split, N=2096, batch_size=32, datamode='triangles', oversampling=4):\n",
    "        self.data_root = data_root\n",
    "        self.batch_size = batch_size\n",
    "        self.N = N\n",
    "        \n",
    "        \n",
    "        metadata = pd.read_csv(os.path.join(self.data_root, 'metadata_modelnet40val.csv'))\n",
    "        \n",
    "        #self.metadata = metadata.loc[metadata.index.repeat(oversampling)]\n",
    "        self.metadata = metadata.loc[metadata.split==split].sample(frac=1)\n",
    "        \n",
    "        \n",
    "        #self.metadata = self.metadata[self.metadata['class'].isin(['xbox', 'airplane'])]\n",
    "        self.labels = dict([(idx, label) for label, idx in enumerate(sorted(self.metadata['class'].unique()))])\n",
    "        \n",
    "        self.datamode = datamode\n",
    "        \n",
    "        self.oversampling = oversampling\n",
    "        self.resampler = TriangleReSampler(self.oversampling*N)\n",
    "        \n",
    "        \n",
    "    def read_off(self, file):\n",
    "        off_header = file.readline().strip()\n",
    "        if 'OFF' == off_header:\n",
    "            n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
    "        else:\n",
    "            n_verts, n_faces, __ = tuple([int(s) for s in off_header[3:].split(' ')])\n",
    "        verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
    "        faces = [[int(idx) for idx in file.readline().strip().split(' ')[1:]] for _ in range(n_faces)]\n",
    "        return tf.constant(verts), tf.ragged.constant(faces)\n",
    "\n",
    "    def load_file_(self, obj_key):\n",
    "        with open(os.path.join(self.data_root, 'ModelNet40', obj_key), 'r') as f:\n",
    "            return self.read_off(f)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.ceil(len(self.metadata)/self.batch_size).astype(int)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        bs = self.batch_size\n",
    "        batch_meta = self.metadata.iloc[idx*bs : (idx+1)*bs]\n",
    "        \n",
    "        ys = batch_meta['class'].apply(lambda z: self.labels[z]).values\n",
    "        \n",
    "        if self.datamode=='points':\n",
    "            X = np.zeros((len(batch_meta), self.N, 3))\n",
    "            mask = np.zeros((len(batch_meta), self.N), dtype=np.int32)\n",
    "\n",
    "            vs = []\n",
    "\n",
    "            for i, path in enumerate(batch_meta.object_path):\n",
    "                verts, _ = self.load_file_(path)\n",
    "\n",
    "                verts = np.unique(verts, axis=0)\n",
    "\n",
    "                if len(verts) > self.N:\n",
    "                    perm = np.random.choice(len(verts), replace=False, size=self.N)\n",
    "                    verts = verts[perm]\n",
    "\n",
    "                mask[i, :len(verts)] = np.ones(len(verts))\n",
    "\n",
    "                X[i, :len(verts)] = verts \n",
    "\n",
    "\n",
    "            return [X, mask], ys\n",
    "        \n",
    "        elif self.datamode =='triangles':\n",
    "            vertl, facel = [], []\n",
    "            \n",
    "            for i, path in enumerate(batch_meta.object_path):\n",
    "                verts, faces = self.load_file_(path)\n",
    "                \n",
    "                vertl.append(verts), facel.append(faces)\n",
    "                \n",
    "            vertrag = tf.ragged.stack(vertl)\n",
    "            facerag = tf.ragged.stack(facel)\n",
    "            \n",
    "            X, M = self.resampler(tf.gather(vertrag, facerag, batch_dims=1).to_tensor())\n",
    "            \n",
    "            X = tf.reshape(X, (len(batch_meta)*self.oversampling, self.N, 3))\n",
    "            M = tf.reshape(M, (len(batch_meta)*self.oversampling, self.N))\n",
    "            \n",
    "            return [X,M], np.repeat(ys, self.oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangleReSampler(Layer):\n",
    "    def __init__(self, N, max_triangles = 5000, **kwargs):\n",
    "        super(TriangleReSampler, self).__init__(**kwargs)\n",
    "        self.N = N\n",
    "        self.max_triangles = 5000\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size = input_shape[0]\n",
    "        return (batch_size, self.N, input_shape[-1])\n",
    "    \n",
    "    def call(self, triangles):\n",
    "        face_count = tf.shape(triangles)[1]\n",
    "        batch_sz = tf.shape(triangles)[0]\n",
    "        \n",
    "        u = triangles[:,:,0]\n",
    "        v = triangles[:,:,1]\n",
    "        w = triangles[:,:,2]\n",
    "        \n",
    "        uv = u-v\n",
    "        wv = w-v\n",
    "\n",
    "        areas = tf.norm(tf.linalg.cross(uv,wv), axis=-1)\n",
    "        sidx = tf.argsort(areas, direction='DESCENDING')[:,:self.max_triangles]\n",
    "        areas = tf.gather(areas, sidx, batch_dims=1)\n",
    "        \n",
    "        dist = tfp.distributions.FiniteDiscrete(tf.range(self.max_triangles), probs=areas)\n",
    "        samp = tfp.distributions.Sample(dist)\n",
    "        \n",
    "        samples = tf.transpose(samp.sample((self.N,)))\n",
    "        samples = tf.gather(sidx, samples, batch_dims=1)\n",
    "        \n",
    "        selected_uv = tf.gather(uv, samples, batch_dims=1)\n",
    "        selected_wv = tf.gather(wv, samples, batch_dims=1)\n",
    "        selected_v = tf.gather(v, samples, batch_dims=1)\n",
    "        \n",
    "        ur = tf.random.uniform((batch_sz, self.N, 1))\n",
    "        wr = tf.random.uniform((batch_sz, self.N, 1))\n",
    "        \n",
    "        coefs = tf.cast(tf.math.ceil(ur+wr)-1, tf.bool)\n",
    "        \n",
    "        urf = tf.where(coefs, 1-ur, ur)\n",
    "        wrf = tf.where(coefs, 1-wr, wr)\n",
    "        \n",
    "        sampled_points = selected_v+selected_uv*urf+selected_wv*wrf\n",
    "        \n",
    "        \n",
    "        return sampled_points, tf.ones((batch_sz, self.N), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchLayer(Layer):\n",
    "    def __init__(self, patch_count, patch_size, patch_radius, **kwargs):\n",
    "        super(PatchLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.patch_radius = patch_radius\n",
    "        self.patch_count = patch_count\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size = input_shape[0]\n",
    "        return [(batch_size, self.patch_count, self.patch_size, 3),\n",
    "                (batch_size, self.patch_count, self.patch_size),\n",
    "                (batch_size, self.patch_count)\n",
    "               ]\n",
    "        \n",
    "        \n",
    "    def call(self, points, mask):\n",
    "        batch_sz = tf.shape(points)[0]\n",
    "        ## rescale\n",
    "        mean, variance = tf.nn.moments(points, 1, keepdims=True)\n",
    "        \n",
    "        max_var = tf.math.reduce_max(variance, axis=2, keepdims=True)+1e-7\n",
    "        \n",
    "        new_points = (points-mean)/tf.sqrt(max_var)\n",
    "        \n",
    "        \n",
    "        # select points\n",
    "        \n",
    "        free_points = tf.cast(tf.identity(mask), dtype=tf.float32)\n",
    "        \n",
    "        outcomes = tf.range(tf.shape(free_points)[1])\n",
    "        distances = None\n",
    "        \n",
    "        \n",
    "        ret_points = tf.TensorArray(tf.float32, size=self.patch_count)\n",
    "        #ret_centers = tf.TensorArray(tf.float32, size=self.patch_count)\n",
    "        ret_mask = tf.TensorArray(tf.int32, size=self.patch_count)\n",
    "        ret_patch_mask = tf.TensorArray(tf.int32, size=self.patch_count)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in tf.range(self.patch_count): \n",
    "            dist = tfp.distributions.FiniteDiscrete(outcomes, probs=free_points)\n",
    "            \n",
    "            samp = tfp.distributions.Sample(dist)\n",
    "            \n",
    "            point_idx = tf.transpose(samp.sample(sample_shape=(1,)))\n",
    "            \n",
    "            principal_points = tf.gather(new_points, point_idx, axis=1, batch_dims=1)\n",
    "            \n",
    "            displacements = principal_points - new_points\n",
    "            \n",
    "            distances = tf.norm(displacements, axis = -1)\n",
    "            \n",
    "            close_enough_points = tf.less(distances, self.patch_radius) # which points are close enough\n",
    "            \n",
    "            valid_points = tf.math.logical_and(close_enough_points, tf.cast(free_points, tf.bool))\n",
    "            \n",
    "            d_idx = tf.argsort(distances) # which are the closest\n",
    "            \n",
    "            point_sel = tf.gather(valid_points, d_idx, batch_dims=1) # \n",
    "            \n",
    "            taken_points = tf.argsort(tf.cast(point_sel, tf.float32), direction='DESCENDING')[:, :self.patch_size] # take the patch_size closest points\n",
    "            \n",
    "            point_mask = tf.cast(tf.gather(point_sel, taken_points, batch_dims=1), tf.int32)\n",
    "            point_indices = tf.gather(d_idx, taken_points, batch_dims = 1)\n",
    "            \n",
    "            batch_points = tf.gather(new_points, point_indices, batch_dims = 1)\n",
    "            \n",
    "            #bmean, _ = tf.nn.moments(batch_points, 1, keepdims=True)\n",
    "            \n",
    "            #batch_points = batch_points - bmean\n",
    "\n",
    "        \n",
    "            used = tf.einsum(\"bce,bc->be\",\n",
    "                           tf.one_hot(point_indices, tf.shape(points)[1]), \n",
    "                           tf.cast(point_mask, tf.float32))\n",
    "            \n",
    "            was_updated = tf.cast(tf.greater(tf.reduce_sum(used, -1), 0.5), tf.int32)\n",
    "            \n",
    "            free_points = free_points-used\n",
    "        \n",
    "            \n",
    "            \n",
    "            ret_points = ret_points.write(i, batch_points)\n",
    "            #ret_centers = ret_centers.write(i, tf.squeeze(bmean))\n",
    "            ret_mask = ret_mask.write(i, point_mask)\n",
    "            ret_patch_mask = ret_patch_mask.write(i, was_updated)\n",
    "        \n",
    "        \n",
    "\n",
    "        return  tf.transpose(ret_points.stack(), perm=[1,0,2,3]), \\\n",
    "                tf.transpose(ret_mask.stack(), perm=[1,0,2]), \\\n",
    "                tf.transpose(ret_patch_mask.stack(), perm=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSPrepadLayer(Layer):\n",
    "    def __init__(self, patch_size, d, **kwargs):\n",
    "        super(CLSPrepadLayer, self).__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.d = d\n",
    "        \n",
    "    def build(self, in_shape):\n",
    "        self.class_patch_emb = self.add_weight(\"cls_pat_emb\", shape=(1, 1, self.patch_size, 3))\n",
    "        #self.class_cent_emb = self.add_weight(\"cls_cnt_emb\", shape=(1, 3))\n",
    "        \n",
    "    def call(self, patches, point_mask, patch_mask):\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "\n",
    "        class_emb = tf.broadcast_to(\n",
    "            self.class_patch_emb, [batch_size, 1, self.patch_size, 3]\n",
    "        )\n",
    "        \"\"\"\n",
    "        class_cent_emb = tf.broadcast_to(\n",
    "            self.class_cent_emb, [batch_size, 1, 3]\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        from_mask = tf.concat([tf.ones((batch_size, 1), dtype=tf.int32), patch_mask], axis=1)\n",
    "        to_mask = tf.concat([tf.zeros((batch_size, 1), dtype=tf.int32), patch_mask], axis=1)\n",
    "        \n",
    "        cls_to_cls_pre = tf.concat([tf.ones((batch_size, 1), dtype=tf.int32), tf.zeros_like(patch_mask)], axis=1)\n",
    "        \n",
    "        cross_mask = tf.einsum('bi,bj->bij', from_mask, to_mask)\n",
    "        cls_to_cls_mask = tf.einsum('bi,bj->bij', cls_to_cls_pre, cls_to_cls_pre)\n",
    "        \n",
    "        \n",
    "        cls_point_mask = tf.ones((batch_size, 1, self.patch_size), dtype=tf.int32)\n",
    "        \n",
    "        return tf.concat([class_emb, patches], axis=1), \\\n",
    "                tf.concat([cls_point_mask, point_mask], axis=1), \\\n",
    "                cross_mask + cls_to_cls_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDDMMSelfAttentionLayer(Layer):\n",
    "    def __init__(self, emb_dim, **kwargs):\n",
    "        super(LDDMMSelfAttentionLayer, self).__init__(**kwargs)\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.Qs = TimeDistributed(Dense(emb_dim))\n",
    "        self.Ks = TimeDistributed(Dense(emb_dim))\n",
    "        self.Vs = TimeDistributed(Dense(emb_dim))\n",
    "        \n",
    "        self.center_attention = MultiHeadAttention(1, emb_dim)\n",
    "        \n",
    "    def call(self, patches, point_mask, attention_mask):\n",
    "\n",
    "        # calculate shape scores\n",
    "        \n",
    "        return patches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingSelfAttentionLayer(Layer):\n",
    "    def __init__(self, emb_dim, **kwargs):\n",
    "        super(PoolingSelfAttentionLayer, self).__init__(**kwargs)\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.att1 = MultiHeadAttention(1, self.emb_dim, value_dim=128)\n",
    "        \n",
    "    def call(self, patches, point_mask, attention_mask):\n",
    "        float_mask = tf.cast(point_mask, tf.float32)\n",
    "        ## zero out with mask\n",
    "        masked = patches*float_mask[:,:,:,tf.newaxis]\n",
    "        sums = tf.reduce_sum(patches, axis=2)\n",
    "        weights = (tf.reduce_sum(float_mask, axis=-1, keepdims=True)+1e-7)\n",
    "        \n",
    "        means = sums/weights\n",
    "        \n",
    "        \n",
    "        return self.att1(means, means, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSelfAttentionLayer(Layer):\n",
    "    def __init__(self, emb_dim, **kwargs):\n",
    "        super(LSTMSelfAttentionLayer, self).__init__(**kwargs)\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.lstm = TimeDistributed(LSTM(emb_dim, recurrent_activation='tanh'))\n",
    "        \n",
    "        self.att = MultiHeadAttention(4, emb_dim)\n",
    "        \n",
    "    def call(self, patches, point_mask, att_mask, training):\n",
    "        \n",
    "        reprs = self.lstm(patches, mask=tf.cast(point_mask, tf.bool), training=training)\n",
    "        \n",
    "        return self.att(reprs, reprs, attention_mask=att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape transformer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embedding_dim, head_count, dropout, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.attention_layer = MultiHeadAttention(head_count, embedding_dim, dropout=dropout)\n",
    "        \n",
    "        self.LN1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.LN2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(TransformerBlock, self).build(input_shape)\n",
    "        insize = input_shape[2]\n",
    "        \n",
    "        self.MLP = Sequential([\n",
    "            Dense(2*insize, activation=tf.keras.activations.gelu),\n",
    "            Dense(insize)\n",
    "        ])\n",
    "        self.MLP.build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, mask, training, return_attention_scores=False):\n",
    "        P = self.LN1(inputs)\n",
    "        if not return_attention_scores:\n",
    "            h1 = self.dropout(self.attention_layer(P, P, attention_mask = mask) + P)\n",
    "\n",
    "            return self.MLP(self.LN2(h1)) + h1\n",
    "        else:\n",
    "            att, scores = self.attention_layer(P, P, attention_mask = mask, return_attention_scores=True)\n",
    "            \n",
    "            h1 = self.dropout(att + P)\n",
    "\n",
    "            return self.MLP(self.LN2(h1)) + h1, scores\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeTransformer1(Model):\n",
    "    def __init__(self, N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=False):\n",
    "        super(ShapeTransformer1, self).__init__()\n",
    "        self.patch_count = patch_count\n",
    "        self.N = N\n",
    "        self.with_resampler = with_resampler\n",
    "        self.patch_emb_dim = patch_emb_dim\n",
    "        \n",
    "        if with_resampler:\n",
    "            self.resampler = TriangleReSampler(N)\n",
    "        \n",
    "        self.patch_layer = PatchLayer(patch_count-1, patch_size, patch_radius)\n",
    "        self.seqproc = CLSPrepadLayer(patch_size, 3)\n",
    "        \n",
    "        self.patch_trans = TimeDistributed(Dense(patch_emb_dim))\n",
    "        \n",
    "        \n",
    "        self.transformerblock1 = TransformerBlock(attention_dim, 2, 0.1)\n",
    "        self.transformerblock2 = TransformerBlock(attention_dim, 2, 0.1)\n",
    "        self.transformerblock3 = TransformerBlock(attention_dim, 2, 0.1)\n",
    "        \n",
    "        self.LN = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense1 = Dense(1024, activation = 'relu')\n",
    "        self.dense2 = Dense(40, activation='softmax')\n",
    "\n",
    "    def summary(self):\n",
    "        if self.with_resampler:\n",
    "            x = Input(shape=(None, 3, 3), name='triangles')\n",
    "            model = Model(inputs=[x, faces], outputs=self.call(x, False))\n",
    "            return model.summary()\n",
    "        else:\n",
    "            x = Input(shape=(self.N, 3), name='points')\n",
    "            mask = Input(shape=(self.N), name='mask')\n",
    "            model = Model(inputs=[x, mask], outputs=self.call([x,mask], False))\n",
    "            return model.summary()\n",
    "    \n",
    "    \n",
    "    def call(self, datin, training):\n",
    "        if self.with_resampler:\n",
    "            triangles = datin\n",
    "            seq, mask = self.resampler(triangles)\n",
    "        else:\n",
    "            seq, mask = datin \n",
    "    \n",
    "        patches, point_mask, patch_mask = self.patch_layer(seq, mask)\n",
    "    \n",
    "        patches, point_mask, attention_mask = self.seqproc(patches, point_mask, patch_mask)\n",
    "        \n",
    "        P = TimeDistributed(Flatten())(patches)\n",
    "        P = self.patch_trans(P)\n",
    "        \n",
    "        x = self.transformerblock1(P, attention_mask, training)\n",
    "        \n",
    "        x = self.transformerblock2(x, attention_mask, training)\n",
    "        \n",
    "        cls = self.LN(x[:, 0])\n",
    "        \n",
    "        x = self.dense1(cls)\n",
    "        \n",
    "        pred = self.dense2(x)\n",
    "    \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1(N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=False):\n",
    "    model = ShapeTransformer1(N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=with_resampler)\n",
    "    \n",
    "    if with_resampler:\n",
    "        model.build(input_shape=(None, None, 3, 3))\n",
    "    else:\n",
    "        model.build(input_shape=[(None, N, 3), (None, N)])\n",
    "    \n",
    "    model.compile('adam', 'sparse_categorical_crossentropy', metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = np.load(\"traindata_medium2048.npz\")\n",
    "X, M, y = trainfile['X'], trainfile['M'], trainfile['y']\n",
    "\n",
    "testfile = np.load(\"testdata_small2048.npz\")\n",
    "Xt, Mt, yt = testfile['X'], testfile['M'], testfile['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws = dict(enumerate(compute_class_weight('balanced', y=y, classes=range(40))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model1(8192, 84, 84, 0.3, 64, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([X,M], y, validation_data=([Xt, Mt], yt), epochs=8*50, class_weight=cws, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape Transformer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeTransformer2(Model):\n",
    "    def __init__(self, N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=False):\n",
    "        super(ShapeTransformer2, self).__init__()\n",
    "        self.patch_count = patch_count\n",
    "        self.N = N\n",
    "        self.with_resampler = with_resampler\n",
    "        self.patch_emb_dim = patch_emb_dim\n",
    "        \n",
    "        if with_resampler:\n",
    "            self.resampler = TriangleReSampler(N)\n",
    "        \n",
    "        self.patch_layer = PatchLayer(patch_count-1, patch_size, patch_radius)\n",
    "        self.seqproc = CLSPrepadLayer(patch_size, 3)\n",
    "        \n",
    "        self.patch_summarizer = TimeDistributed(Bidirectional(LSTM(patch_emb_dim//2)), name=\"PatchLSTM\")\n",
    "        \n",
    "        \n",
    "        self.transformerblock1 = TransformerBlock(attention_dim, 2, 0.1, name=\"t1\")\n",
    "        self.transformerblock2 = TransformerBlock(attention_dim, 2, 0.1, name=\"t2\")\n",
    "        self.transformerblock3 = TransformerBlock(attention_dim, 2, 0.1)\n",
    "        \n",
    "        self.LN = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense1 = Dense(1024, activation = 'relu')\n",
    "        self.dense2 = Dense(40, activation='softmax')\n",
    "\n",
    "    def summary(self):\n",
    "        if self.with_resampler:\n",
    "            x = Input(shape=(None, 3, 3), name='triangles')\n",
    "            model = Model(inputs=[x, faces], outputs=self.call(x, False))\n",
    "            return model.summary()\n",
    "        else:\n",
    "            x = Input(shape=(self.N, 3), name='points')\n",
    "            mask = Input(shape=(self.N), name='mask')\n",
    "            model = Model(inputs=[x, mask], outputs=self.call([x,mask], False))\n",
    "            return model.summary()\n",
    "    \n",
    "    \n",
    "    def call(self, datin, training):\n",
    "        if self.with_resampler:\n",
    "            triangles = datin\n",
    "            seq, mask = self.resampler(triangles)\n",
    "        else:\n",
    "            seq, mask = datin \n",
    "    \n",
    "        patches, point_mask, patch_mask = self.patch_layer(seq, mask)\n",
    "    \n",
    "        patches, point_mask, attention_mask = self.seqproc(patches, point_mask, patch_mask)\n",
    "        \n",
    "        P = self.patch_summarizer(patches, point_mask)\n",
    "        \n",
    "        x = self.transformerblock1(P, attention_mask, training)\n",
    "        \n",
    "        x = self.transformerblock2(x, attention_mask, training)\n",
    "        \n",
    "        cls = self.LN(x[:, 0])\n",
    "        \n",
    "        x = self.dense1(cls)\n",
    "        \n",
    "        pred = self.dense2(x)\n",
    "    \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=False):\n",
    "    model = ShapeTransformer2(N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=with_resampler)\n",
    "    \n",
    "    if with_resampler:\n",
    "        model.build(input_shape=(None, None, 3, 3))\n",
    "    else:\n",
    "        model.build(input_shape=[(None, N, 3), (None, N)])\n",
    "    \n",
    "    model.compile('adam', 'sparse_categorical_crossentropy', metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model2(512, 64, 32, 0.3, 64, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit([X,M], y, validation_data=([Xt, Mt], yt), epochs=8*50, class_weight=cws, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape Transformer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, emb_dim, **kwargs):\n",
    "        super(PatchEncoder, self).__init__(**kwargs)\n",
    "        self.flattener = TimeDistributed(Flatten())\n",
    "        self.point_projection = TimeDistributed(Dense(emb_dim))\n",
    "        self.positional_encoder = TimeDistributed(Dense(emb_dim))\n",
    "        \n",
    "    def call(self, sequence, point_mask):\n",
    "        means = tf.einsum('bmpd,bmp->bmd', sequence, tf.cast(point_mask, tf.float32))\n",
    "        \n",
    "        exmeans = tf.expand_dims(means, 2)\n",
    "        \n",
    "        centered = sequence-exmeans\n",
    "        \n",
    "        flattened = self.flattener(centered)\n",
    "        \n",
    "        shape_encoding = self.point_projection(flattened)\n",
    "        \n",
    "        positional_encoding = self.positional_encoder(means)\n",
    "        \n",
    "        return shape_encoding+positional_encoding\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeTransformer3(Model):\n",
    "    def __init__(self, N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=False):\n",
    "        super(ShapeTransformer3, self).__init__()\n",
    "        self.patch_count = patch_count\n",
    "        self.N = N\n",
    "        self.with_resampler = with_resampler\n",
    "        self.patch_emb_dim = patch_emb_dim\n",
    "        \n",
    "        if with_resampler:\n",
    "            self.resampler = TriangleReSampler(N)\n",
    "        \n",
    "        self.patch_layer = PatchLayer(patch_count-1, patch_size, patch_radius)\n",
    "        self.seqproc = CLSPrepadLayer(patch_size, 3)\n",
    "        self.patch_encoder = PatchEncoder(patch_emb_dim)\n",
    "        \n",
    "        \n",
    "        self.transformerblock1 = TransformerBlock(attention_dim, 2, 0.1, name=\"t1\")\n",
    "        self.transformerblock2 = TransformerBlock(attention_dim, 2, 0.1, name=\"t2\")\n",
    "        self.transformerblock3 = TransformerBlock(attention_dim, 2, 0.1)\n",
    "        \n",
    "        self.LN = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense1 = Dense(1024, activation = 'relu')\n",
    "        self.dense2 = Dense(40, activation='softmax')\n",
    "\n",
    "    def summary(self):\n",
    "        if self.with_resampler:\n",
    "            x = Input(shape=(None, 3, 3), name='triangles')\n",
    "            model = Model(inputs=[x, faces], outputs=self.call(x, False))\n",
    "            return model.summary()\n",
    "        else:\n",
    "            x = Input(shape=(self.N, 3), name='points')\n",
    "            mask = Input(shape=(self.N), name='mask')\n",
    "            model = Model(inputs=[x, mask], outputs=self.call([x,mask], False))\n",
    "            return model.summary()\n",
    "    \n",
    "    \n",
    "    def call(self, datin, training):\n",
    "        if self.with_resampler:\n",
    "            triangles = datin\n",
    "            seq, mask = self.resampler(triangles)\n",
    "        else:\n",
    "            seq, mask = datin \n",
    "    \n",
    "        patches, point_mask, patch_mask = self.patch_layer(seq, mask)\n",
    "    \n",
    "        patches, point_mask, attention_mask = self.seqproc(patches, point_mask, patch_mask)\n",
    "        \n",
    "        P = self.patch_encoder(patches, point_mask)\n",
    "        \n",
    "        x = self.transformerblock1(P, attention_mask, training)\n",
    "        \n",
    "        x = self.transformerblock2(x, attention_mask, training)\n",
    "        \n",
    "        cls = self.LN(x[:, 0])\n",
    "        \n",
    "        x = self.dense1(cls)\n",
    "        \n",
    "        pred = self.dense2(x)\n",
    "    \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model3(N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=False):\n",
    "    model = ShapeTransformer3(N, patch_count, patch_size, patch_radius, attention_dim, patch_emb_dim, with_resampler=with_resampler)\n",
    "    \n",
    "    if with_resampler:\n",
    "        model.build(input_shape=(None, None, 3, 3))\n",
    "    else:\n",
    "        model.build(input_shape=[(None, N, 3), (None, N)])\n",
    "    \n",
    "    model.compile('adam', 'sparse_categorical_crossentropy', metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model3(512, 64, 32, 0.3, 64, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit([X,M], y, validation_data=([Xt, Mt], yt), epochs=8*50, class_weight=cws, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeTransformer(Model):\n",
    "    def __init__(self, N, transformer_count, transformer_dropout, \n",
    "                         t_headcount,\n",
    "                         patch_count, patch_size, \n",
    "                         patch_radius, attention_dim, \n",
    "                         patch_emb_dim, with_resampler=False):\n",
    "        super(ShapeTransformer, self).__init__()\n",
    "\n",
    "        self.patch_count = patch_count\n",
    "        self.N = N\n",
    "        self.with_resampler = with_resampler\n",
    "        self.patch_emb_dim = patch_emb_dim\n",
    "        self.headcount = t_headcount\n",
    "        \n",
    "        if with_resampler:\n",
    "            self.resampler = TriangleReSampler(N)\n",
    "        \n",
    "        self.patch_layer = PatchLayer(patch_count-1, patch_size, patch_radius)\n",
    "        self.seqproc = CLSPrepadLayer(patch_size, 3)\n",
    "        \n",
    "        self.patch_trans = TimeDistributed(Dense(patch_emb_dim))\n",
    "        \n",
    "        assert transformer_count > 0\n",
    "        self.attblocks = [TransformerBlock(attention_dim, t_headcount, transformer_dropout, name=f\"transformer_block_{i}\") for i in range(transformer_count)]\n",
    "        \n",
    "        self.LN = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense1 = Dense(2048, activation = 'relu')\n",
    "        self.dropout1 = Dropout(0.2)\n",
    "        self.dense2 = Dense(1024)\n",
    "        self.dense3 = Dense(40, activation='softmax')\n",
    "\n",
    "    def summary(self):\n",
    "        if self.with_resampler:\n",
    "            x = Input(shape=(None, 3, 3), name='triangles')\n",
    "            model = Model(inputs=[x, faces], outputs=self.call(x, False))\n",
    "            return model.summary()\n",
    "        else:\n",
    "            x = Input(shape=(self.N, 3), name='points')\n",
    "            mask = Input(shape=(self.N), name='mask')\n",
    "            model = Model(inputs=[x, mask], outputs=self.call([x,mask], False))\n",
    "            return model.summary()\n",
    "    \n",
    "    \n",
    "    def call(self, datin, training):\n",
    "        if self.with_resampler:\n",
    "            triangles = datin\n",
    "            seq, mask = self.resampler(triangles)\n",
    "        else:\n",
    "            seq, mask = datin \n",
    "    \n",
    "        patches, point_mask, patch_mask = self.patch_layer(seq, mask)\n",
    "    \n",
    "        patches, point_mask, attention_mask = self.seqproc(patches, point_mask, patch_mask)\n",
    "        \n",
    "        P = TimeDistributed(Flatten())(patches)\n",
    "        x = self.patch_trans(P)\n",
    "        \n",
    "        for block in self.attblocks:\n",
    "            x = block(x, attention_mask, training)\n",
    "        \n",
    "        cls = self.LN(x[:, 0])\n",
    "        \n",
    "        x = self.dense1(cls)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        pred = self.dense3(x)\n",
    "    \n",
    "        return pred\n",
    "    \n",
    "    def inspect(self, datin):\n",
    "        if self.with_resampler:\n",
    "            triangles = datin\n",
    "            seq, mask = self.resampler(triangles)\n",
    "        else:\n",
    "            seq, mask = datin \n",
    "    \n",
    "        patches, point_mask, patch_mask = self.patch_layer(seq, mask)\n",
    "    \n",
    "        patches, point_mask, attention_mask = self.seqproc(patches, point_mask, patch_mask)\n",
    "        \n",
    "        P = TimeDistributed(Flatten())(patches)\n",
    "        x = self.patch_trans(P)\n",
    "        \n",
    "        bs = tf.shape(seq)[0]\n",
    "        \n",
    "        attscores = np.zeros((len(self.attblocks), bs, self.headcount, self.patch_count, self.patch_count))\n",
    "        \n",
    "        for i, block in enumerate(self.attblocks):\n",
    "            x, score = block(x, attention_mask, False, return_attention_scores = True)\n",
    "            attscores[i] = score\n",
    "        \n",
    "        cls = self.LN(x[:, 0])\n",
    "        \n",
    "        x = self.dense1(cls)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        pred = self.dense3(x)\n",
    "    \n",
    "        return pred, attscores, patches, point_mask, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(N, transformer_count, transformer_dropout, \n",
    "                 t_headcount, \n",
    "                 patch_count, patch_size, \n",
    "                 patch_radius, attention_dim, patch_emb_dim, optimizer='adam', with_resampler=False):\n",
    "    model = ShapeTransformer(N,transformer_count, transformer_dropout, \n",
    "                                t_headcount, patch_count, \n",
    "                                patch_size, patch_radius, attention_dim, \n",
    "                                patch_emb_dim, with_resampler=with_resampler)\n",
    "    \n",
    "    if with_resampler:\n",
    "        model.build(input_shape=(None, None, 3, 3))\n",
    "    else:\n",
    "        model.build(input_shape=[(None, N, 3), (None, N)])\n",
    "    \n",
    "    model.compile(optimizer, 'sparse_categorical_crossentropy', metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hypermodel1(hp, *, log_dir, N):\n",
    "    hN = hp.Fixed(\"N\", N)\n",
    "    \n",
    "    htrans_count = hp.Int(\"transformer_count\", min_value=1, max_value=6)\n",
    "    \n",
    "    htransformer_dropout = hp.Float(\"transformer_dropout\", min_value=0, max_value=0.5, step=0.1)\n",
    "    htransformer_headcount = hp.Int(\"transformer_headcount\", min_value=1, max_value=4)\n",
    "    \n",
    "    hpatch_count = hp.Int(\"patch_count\", min_value=8, max_value=N//16, step=8)\n",
    "    hpatch_size =  N//hpatch_count\n",
    "    \n",
    "    hpatch_radius = hp.Fixed(\"patch_radius\", 0.2)\n",
    "    \n",
    "    hattention_dim = hp.Int(\"attention_dim\", min_value=16, max_value=512, step=16)\n",
    "    hpatch_emb_dim = hp.Int(\"patch_dim\", min_value=16, max_value=512, step=16)\n",
    "\n",
    "    hlr = hp.Float(\"learning_rate\", min_value=5e-5, max_value=5e-3, sampling='log')\n",
    "\n",
    "    \n",
    "    if hp.Boolean(\"use_decay\"):\n",
    "        hlr = CosineDecay(hlr, 1500)\n",
    "    \n",
    "    otim = Adam(learning_rate=hlr)\n",
    "    \n",
    "    return create_model(\n",
    "        N=hN,\n",
    "        transformer_count = htrans_count,\n",
    "        transformer_dropout = htransformer_dropout,\n",
    "        t_headcount = htransformer_headcount,\n",
    "        patch_count = hpatch_count,\n",
    "        patch_size = hpatch_size,\n",
    "        patch_radius = hpatch_radius,\n",
    "        attention_dim = hattention_dim,\n",
    "        patch_emb_dim = hpatch_emb_dim,\n",
    "        optimizer = otim,\n",
    "        with_resampler=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_run(N, factory):\n",
    "    \n",
    "    run_name = input(\"Run name: \")\n",
    "    log_dir = os.path.join(\"logs\", run_name)\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "    \n",
    "    model_factory = lambda hp: factory(hp, log_dir=log_dir, N=N)\n",
    "    \n",
    "    trainfile = np.load(f\"traindata_medium{N}NEW.npz\")\n",
    "    X, M, y = trainfile['X'], trainfile['M'], trainfile['y']\n",
    "\n",
    "    valfile = np.load(f\"validationdata_small{N}.npz\")\n",
    "    Xv, Mv, yv = valfile['X'], valfile['M'], valfile['y']\n",
    "\n",
    "    \n",
    "    cws = dict(enumerate(compute_class_weight('balanced', y=y, classes=range(40))))\n",
    "    \n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=0.01),\n",
    "        TensorBoard(log_dir, histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        model_factory,\n",
    "        objective='val_loss',\n",
    "        factor=3,\n",
    "        max_epochs=20,\n",
    "        directory=os.path.join(log_dir, \"tuner_dir\"),\n",
    "        project_name=\"ST1\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    tuner.search_space_summary()\n",
    "\n",
    "\n",
    "    tuner.search(\n",
    "        x=[X,M], y=y, \n",
    "        validation_data=([Xv, Mv], yv),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=cws,\n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tun = tuning_run(2048, create_hypermodel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tun.get_best_hyperparameters(1)[0][\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hypermodellr(hp, *, log_dir, N):\n",
    "    hN = hp.Fixed(\"N\", N)\n",
    "    \n",
    "    htrans_count = hp.Fixed(\"transformer_count\", 3)\n",
    "    \n",
    "    htransformer_dropout = hp.Fixed(\"transformer_dropout\", 0.1)\n",
    "    htransformer_headcount = hp.Fixed(\"transformer_headcount\", 3)\n",
    "    \n",
    "    hpatch_count = hp.Fixed(\"patch_count\", 128)\n",
    "    hpatch_size =  N//hpatch_count\n",
    "    \n",
    "    hpatch_radius = hp.Fixed(\"patch_radius\", 0.2)\n",
    "    \n",
    "    hattention_dim = hp.Fixed(\"attention_dim\", 84)\n",
    "    hpatch_emb_dim = hp.Fixed(\"patch_dim\", 96)\n",
    "\n",
    "    hlr = hp.Float(\"learning_rate\", min_value=5e-5, max_value=5e-3, sampling='log')\n",
    "    hmom = hp.Float(\"momentum\", min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    if hp.Boolean(\"use_decay\"):\n",
    "        hlr = CosineDecay(hlr, 500)\n",
    "    \n",
    "    otim = RMSprop(learning_rate=hlr, momentum=hmom)\n",
    "    \n",
    "    return create_model(\n",
    "        N=hN,\n",
    "        transformer_count = htrans_count,\n",
    "        transformer_dropout = htransformer_dropout,\n",
    "        t_headcount = htransformer_headcount,\n",
    "        patch_count = hpatch_count,\n",
    "        patch_size = hpatch_size,\n",
    "        patch_radius = hpatch_radius,\n",
    "        attention_dim = hattention_dim,\n",
    "        patch_emb_dim = hpatch_emb_dim,\n",
    "        optimizer = otim,\n",
    "        with_resampler=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_run(2048, create_hypermodellr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(        \n",
    "    N=2048,\n",
    "    transformer_count = 3,\n",
    "    transformer_dropout = 0.1,\n",
    "    t_headcount = 3,\n",
    "    patch_count = 128,\n",
    "    patch_size = 2048//128,\n",
    "    patch_radius = 0.2,\n",
    "    attention_dim = 84,\n",
    "    patch_emb_dim = 96,\n",
    "    optimizer = Adam(0.0007),\n",
    "    with_resampler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = np.load(\"traindata_medium2048NEW.npz\")\n",
    "X, M, y = trainfile['X'], trainfile['M'], trainfile['y']\n",
    "\n",
    "valfile = np.load(\"validationdata_small2048.npz\")\n",
    "Xv, Mv, yv = valfile['X'], valfile['M'], valfile['y']\n",
    "\n",
    "testfile = np.load(\"testdata_small2048.npz\")\n",
    "Xt, Mt, yt = testfile['X'], testfile['M'], testfile['y']\n",
    "\n",
    "cws = dict(enumerate(compute_class_weight('balanced', y=y, classes=range(40))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5, min_delta=0.01, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "model.fit([X,M], y, \n",
    "          validation_data=([Xv, Mv], yv), \n",
    "          epochs=50, class_weight=cws, batch_size=256, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([Xt, Mt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conmat = confusion_matrix(yt, np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, ml, yl = Xt[:16], Mt[:16], yt[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred, atts, patches, pontmask, attmask = model.inspect([xl, ml])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(15,15)\n",
    "ax = []\n",
    "pw = 14\n",
    "\n",
    "p = patches[pw, 1:]\n",
    "for layer in range(3):\n",
    "    for head in range(3):\n",
    "    \n",
    "        ax = fig.add_subplot(3,3,3*layer+head+1, projection='3d')\n",
    "\n",
    "        \n",
    "        colors = atts[layer, pw, head, 0, 1:]\n",
    "\n",
    "        colors /= max(colors)\n",
    "        ax.set_title(next(k for (k,v) in mn.labels.items() if v == yl[pw]))\n",
    "\n",
    "        for j in range(127):\n",
    "            ax.scatter(*p[j].numpy().T, c=plt.cm.RdYlGn(np.repeat(colors[j], 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(15,15)\n",
    "ax = []\n",
    "h = 1\n",
    "for i in range(9):\n",
    "    \n",
    "    ax = fig.add_subplot(3,3,i+1, projection='3d')\n",
    "    \n",
    "    p = patches[i, 1:]\n",
    "    colors = atts[h, i, 0, 0, 1:]\n",
    "    \n",
    "    colors /= max(colors)\n",
    "    ax.set_title(next(k for (k,v) in mn.labels.items() if v == yl[i]))\n",
    "    \n",
    "    for j in range(127):\n",
    "        ax.scatter(*p[j].numpy().T, c=plt.cm.jet(np.repeat(colors[j], 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
